[tool.poetry]
name = "til24-nlp"
version = "0.1.0"
description = "template for python api server"
authors = ["Your Name <you@example.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "<3.13,>=3.10"
rich = "^13.7.1"
fastapi = { extras = ["all"], version = "^0.111.0" }
python-dotenv = "^1.0.1"
instructor = "^1.2.6"
llama-cpp-python = [
  { version = "^0.2.74", extras = [
    "server",
  ], source = "llama-cpp-python-cuda", markers = "sys_platform == 'win32' or sys_platform == 'linux'" },
  { version = "^0.2.74", extras = [
    "server",
  ], source = "llama-cpp-python-metal", markers = "sys_platform == 'darwin'" },
]
nvidia-cuda-runtime-cu12 = { version = "^12.4.127", markers = "sys_platform == 'linux'" }
nvidia-cublas-cu12 = { version = "^12.4.5.8", markers = "sys_platform == 'linux'" }

[tool.poetry.group.dev.dependencies]
ruff = "*"
poethepoet = "*"
rouge-score = "^0.1.2"
pandas = "^2.2.2"
requests = "^2.31.0"

[[tool.poetry.source]]
name = "llama-cpp-python-cuda"
url = "https://abetlen.github.io/llama-cpp-python/whl/cu124"
priority = "explicit"

[[tool.poetry.source]]
name = "llama-cpp-python-metal"
url = "https://abetlen.github.io/llama-cpp-python/whl/metal"
priority = "explicit"

[tool.poe.tasks]
dev = "python dev.py"
prod = "docker run --rm --gpus all -p 5002:5002 nyanplan3-nlp"
test = "python eval/test_nlp.py"
publish = "docker push asia-southeast1-docker.pkg.dev/dsta-angelhack/repository-nyanplan3/nyanplan3-nlp:latest"

[tool.poe.tasks.build]
cmd = """docker build -f Dockerfile . \
  -t nyanplan3-nlp:latest \
  -t asia-southeast1-docker.pkg.dev/dsta-angelhack/repository-nyanplan3/nyanplan3-nlp:latest \
  -t nyanplan3-nlp:${tag} \
  -t asia-southeast1-docker.pkg.dev/dsta-angelhack/repository-nyanplan3/nyanplan3-nlp:${tag} \
"""
args = [{ name = "tag", positional = true, required = true }]

[tool.ruff.lint]
select = ["D"]

[tool.ruff.lint.pydocstyle]
convention = "google"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
